how to train an XG boost model in Python( time: 0.78 ) this tutorial will get you started with( time: 3.899 ) an example step by step( time: 5.94 ) I'll show you what is xq Boost and how( time: 8.099 ) to implement it in Python including( time: 10.559 ) scikit-learn pipeline setup hyper( time: 12.719 ) parameter tuning model evaluation and( time: 14.82 ) feature importance plot( time: 17.58 ) by the end you'll be able to build your( time: 19.32 ) own xq boost model for your prediction( time: 21.359 ) tasks let's Dive In( time: 23.22 ) hi everyone I'm Justin welcome to just( time: 25.98 ) into Data where data science materials( time: 28.619 ) are shared and made simpler for you( time: 30.72 ) before jumping into the example in( time: 33.0 ) Python let's answer the question what is( time: 34.98 ) xq Boost( time: 38.04 ) xgboost stands for extreme gradient( time: 39.5 ) boosting it's an optimized( time: 42.239 ) implementation of the gradient boosting( time: 44.399 ) algorithm( time: 46.44 ) it became well known because of its( time: 47.52 ) outstanding accuracy and efficiency( time: 49.739 ) compared to other algorithms in machine( time: 51.539 ) learning competitions( time: 54.0 ) who can easily apply xgboost for( time: 56.28 ) supervised learning problems to make( time: 58.5 ) predictions( time: 60.239 ) in case you're not familiar with( time: 61.559 ) gradient boosting let's also briefly( time: 63.42 ) explain it( time: 65.82 ) gradient boosting is a machine learning( time: 67.2 ) algorithm that sequentially ensembles( time: 69.54 ) weak predictive models into a single( time: 71.939 ) stronger predictive model( time: 74.34 ) we can apply it to both supervised( time: 76.68 ) regression and classification problems( time: 78.72 ) if you're more interested in learning( time: 81.96 ) about the gradient boosting algorithm( time: 83.64 ) we've covered its fundamentals in a( time: 85.92 ) written tutorial please check out what( time: 88.2 ) is gradient boosting and machine( time: 90.54 ) learning fundamentals explained( time: 91.92 ) I'll put the link in the description( time: 94.799 ) so the most common choice of weak models( time: 97.56 ) to Ensemble ingredient boosting are( time: 100.2 ) decision trees( time: 102.479 ) the tree-based gradient boosting methods( time: 104.22 ) are performant and easy to use( time: 106.5 ) despite the advantages of tree based( time: 109.979 ) gradient boosting methods the model( time: 112.32 ) tends to overfit as well as could demand( time: 114.42 ) a lot of computing power( time: 117.119 ) xgboost also a tree based gradient( time: 119.299 ) boosting implementation overcomes some( time: 122.1 ) disadvantages with its optimizations( time: 124.92 ) let's look at some key improvements of( time: 128.16 ) xgboost versus traditional gradient( time: 130.319 ) boosting( time: 132.48 ) it is more regularized( time: 133.68 ) xgboost uses a more regularized model( time: 136.4 ) formalization to control overfitting( time: 139.379 ) which gives it better performance( time: 142.14 ) you can consider xgboost as a more( time: 145.379 ) regularized version of gradient tree( time: 147.959 ) boosting( time: 149.94 ) for example( time: 151.5 ) the objective function of xgboost has a( time: 152.879 ) regularization term added to the loss( time: 156.0 ) function( time: 158.34 ) we'll use some regularization parameters( time: 159.78 ) in our xgboost python example( time: 162.18 ) also more scalable( time: 165.66 ) xgboost was designed to be scalable( time: 168.42 ) it has implemented practices such as( time: 171.599 ) memory optimization cache optimization( time: 173.819 ) and distributed computing that can( time: 176.7 ) handle large data sets( time: 179.28 ) so overall xgboost is a faster framework( time: 181.56 ) that can build better models( time: 184.8 ) the xgboost framework has an open source( time: 188.4 ) python package( time: 191.459 ) this package was built with easy( time: 193.5 ) integration with the popular machine( time: 195.78 ) learning library scikit-learn( time: 198.06 ) if you're familiar with scikit-learn you( time: 200.879 ) may find it easier to use xgboost( time: 203.4 ) alright now we're ready to build an xq( time: 206.58 ) boost model in Python( time: 209.04 ) I'll use jupyter lab to demonstrate this( time: 211.26 ) tutorial( time: 213.48 ) in this notebook I've divided the( time: 214.92 ) process into different steps( time: 217.26 ) Step 1 explore and prep data( time: 219.959 ) we'll use some Bank marketing data as an( time: 223.799 ) example( time: 226.5 ) you can download the data set from this( time: 227.94 ) UCI page( time: 230.28 ) Link in the description( time: 231.78 ) this data set records a telemarketing( time: 234.36 ) campaigns of a Portuguese Bank( time: 237.0 ) based on the client information( time: 240.599 ) and campaign activities( time: 243.42 ) will predict if a client will subscribe( time: 245.7 ) to a term deposit either yes or no( time: 248.04 ) so we're dealing with a supervised( time: 252.12 ) classification task( time: 253.86 ) this project comes with different data( time: 256.32 ) sets( time: 258.66 ) we'll use this one Bank additional foe( time: 259.859 ) dot CSV( time: 262.86 ) so let's click the data folder( time: 265.259 ) you can download this Bank( time: 269.34 ) additional.zip file( time: 270.8 ) don't forget to extract it to your( time: 273.419 ) desired location since it's zipped( time: 275.1 ) the CSV file is contained in this folder( time: 277.68 ) back to the notebook( time: 281.4 ) let's load the data set( time: 283.86 ) as usual we'll import pandas as PD and( time: 286.139 ) use a read CSV function to load the data( time: 289.919 ) set as DF( time: 292.259 ) in this XT boost Python tutorial I( time: 295.62 ) assume you already know the basics of( time: 298.38 ) python including pandas( time: 299.94 ) if you need help please check out our( time: 302.46 ) course( time: 304.8 ) python for data analysis step by step( time: 305.639 ) with projects( time: 308.22 ) this course teaches pandas which is( time: 310.56 ) necessary to transform your data set( time: 313.08 ) before modeling and much more( time: 314.82 ) again Link in the description below( time: 317.639 ) alright back to our code( time: 321.06 ) in reality after loading the data we'd( time: 323.82 ) explore the data set before transforming( time: 327.0 ) it( time: 328.919 ) but for Simplicity I'll skip the process( time: 329.759 ) here and change the data set with the( time: 332.34 ) following code( time: 334.56 ) so this variable calls to drop( time: 336.18 ) stores The Columns to drop( time: 339.06 ) the ones that are less related to the( time: 341.039 ) Target based on my judgment( time: 342.96 ) then we'll transform the data set DF by( time: 345.419 ) dropping this list of columns( time: 348.72 ) and at the same time rename the rest of( time: 350.639 ) the columns so that they're more( time: 353.16 ) understandable( time: 354.78 ) for example we rename the column job as( time: 356.52 ) job type( time: 360.12 ) default as default status and so on( time: 361.32 ) for the detailed definition of these( time: 366.18 ) columns( time: 368.1 ) please read the UCI page of the data set( time: 369.0 ) that was shown earlier( time: 371.52 ) lastly we convert the target the column( time: 373.86 ) as result( time: 376.919 ) to numerical values( time: 378.24 ) so when yes( time: 380.22 ) the client accepts a term deposit the( time: 381.66 ) value is one otherwise zero( time: 384.06 ) now let's look at our clean data set( time: 388.56 ) we can print out the first five rows( time: 392.039 ) so here is the head of the data set( time: 395.4 ) you can have a quick look( time: 397.86 ) also we can print its info summary( time: 404.4 ) as you can see here we have 14 features( time: 408.419 ) and the target result( time: 411.78 ) with no missing data( time: 413.94 ) note that xgboost can handle missing( time: 417.84 ) values internally even if there are some( time: 419.94 ) and if we look at the value counts of( time: 423.3 ) the result column( time: 425.639 ) it shows that most of the customers( time: 427.319 ) rejected the offer from the bank( time: 429.12 ) while( time: 431.639 ) 4640 accepted it( time: 432.62 ) all right I won't spend too much time( time: 435.3 ) looking at the data set( time: 437.759 ) next let's split the data set into( time: 440.22 ) training and test sets as in the usual( time: 442.74 ) machine learning process( time: 445.74 ) we first separate the features from the( time: 447.96 ) target as X and Y( time: 450.539 ) then we separate the x's and the Y's for( time: 453.78 ) training and test sets using the train( time: 457.56 ) test split function from sklearn( time: 460.02 ) we also set the sampling to be( time: 463.74 ) stratified based on the target's value( time: 465.9 ) and also a random number seed so that we( time: 468.9 ) can get a reproducible result( time: 471.479 ) now we have the training set as X train( time: 474.0 ) and Y train( time: 476.819 ) which is 80 percent of the original set( time: 478.56 ) and the test set as X test and Y test( time: 481.62 ) which is 20 of the original data set( time: 485.099 ) that's how the data prep will do for( time: 490.319 ) this tutorial( time: 492.3 ) next in Step 2 we'll set up a pipeline( time: 493.979 ) of training using the scikit-learn( time: 497.52 ) package( time: 499.5 ) the scikit-learn pipeline can( time: 501.599 ) sequentially apply a list of transforms( time: 503.58 ) and a final estimator( time: 505.8 ) it conveniently assembles several steps( time: 508.979 ) or changes that can be cross-validated( time: 511.56 ) together when training( time: 513.899 ) building a pipeline is much easier and( time: 517.26 ) ensures consistency than setting up the( time: 519.779 ) process manually( time: 521.82 ) hence it's a good practice to follow( time: 523.62 ) if you're not familiar with the pipeline( time: 526.92 ) you can find the link to this page below( time: 528.779 ) and read more about it( time: 530.94 ) so back to our example here( time: 533.64 ) let's set up a pipeline called pipe( time: 536.16 ) holding the parameter steps as this( time: 538.92 ) variable called estimators( time: 541.14 ) the estimators includes a list of tuples( time: 543.959 ) in sequential order( time: 546.6 ) first an encoder of Target encoder( time: 549.54 ) encoding like this is a standard( time: 553.5 ) pre-processing procedure and( time: 555.72 ) classification prediction problems( time: 557.76 ) it will transform the categorical( time: 560.399 ) features in our data set into numeric( time: 562.26 ) ones( time: 564.779 ) you can read more about Target encoders( time: 566.22 ) which I'll put a link below( time: 568.32 ) then an estimator clf of X Cube boost( time: 570.6 ) classifier( time: 574.5 ) this is the SK learn wrapper( time: 576.3 ) implementation of the xq Boost( time: 578.04 ) classification( time: 580.32 ) again we have the random State set here( time: 582.6 ) to get reproducible results( time: 585.54 ) if you're having a regression problem( time: 589.019 ) please use xgb regressor instead( time: 590.7 ) if we run the code and print out pipe( time: 594.839 ) actually you can see some warning( time: 598.44 ) messages( time: 600.36 ) but they're nothing we need to worry( time: 601.5 ) about for now( time: 602.88 ) we'll just ignore them( time: 604.26 ) Below in the output we can see that( time: 606.6 ) we've assigned the pipeline steps as( time: 609.0 ) encoder followed by clf( time: 611.279 ) in the following steps we'll train the( time: 616.5 ) data set by calling this pipeline to( time: 618.839 ) ensure the data set is always encoded by( time: 621.72 ) Target encoder before fitting the xq( time: 624.18 ) Boost classifier( time: 627.0 ) all right moving on step three( time: 629.22 ) one more very important step before( time: 633.54 ) training our xcubus model in Python( time: 635.7 ) the extra boost model contains many( time: 638.94 ) hyper parameters( time: 641.04 ) we should tune them to get a better( time: 642.72 ) estimate of the model( time: 644.16 ) as you might know there are different( time: 646.14 ) ways of hyper parameter tuning such as( time: 648.12 ) grid search and random search( time: 650.7 ) Instead This tutorial will use a( time: 653.459 ) different approach( time: 656.04 ) we'll use a package called scikit( time: 658.2 ) optimize SK opt for hyper parameter( time: 660.48 ) tuning( time: 663.72 ) it's easy to use and integrates easily( time: 664.92 ) with scikit-learn( time: 667.74 ) within the package we'll use an object( time: 669.779 ) called base search CV( time: 672.12 ) the scikit-learn hyperparameter search( time: 675.3 ) wrapper( time: 677.399 ) in short it utilizes Bayesian( time: 679.74 ) optimization where a predictive model is( time: 682.2 ) used to Model A search space of( time: 685.019 ) hyperparameter values to arrive at a( time: 687.0 ) good value combination based on Crouch( time: 689.64 ) validation performance( time: 692.1 ) so it's an efficient yet effective( time: 695.04 ) approach to hyper parameter tuning( time: 697.14 ) to use the space search CV method we( time: 699.839 ) need to define a search space of hyper( time: 703.26 ) parameter values( time: 705.3 ) back to our notebook( time: 707.399 ) in the python code below( time: 709.92 ) within the variable search space( time: 712.019 ) we set up the ranges of the selected( time: 714.6 ) hyper parameter values that will be( time: 716.64 ) searched as a dictionary( time: 718.8 ) the keys of the dictionary are parameter( time: 721.86 ) names( time: 724.92 ) in our case we're using a pipeline( time: 726.66 ) so first we specify the name of the xgb( time: 729.959 ) classifier estimator clf( time: 733.32 ) that we've set up earlier( time: 737.22 ) followed by two underscores( time: 743.7 ) and the hyper parameter name( time: 746.82 ) this is a structure of how to call( time: 749.7 ) nested parameters within a pipeline( time: 751.68 ) again the estimator name of clf( time: 755.04 ) referring to xqb classifier( time: 758.42 ) two underscores followed by the( time: 761.339 ) parameter name( time: 763.92 ) for example( time: 766.2 ) this is calling within the estimator( time: 767.519 ) called clf the parameter name max depth( time: 769.74 ) this is a learning rate and so on( time: 774.36 ) then the values of the dictionary are( time: 779.16 ) the type and range of the hyperparameter( time: 782.339 ) defined by the space module of scikit( time: 785.459 ) optimize( time: 788.279 ) we have options of integer real or( time: 790.8 ) categorical( time: 794.7 ) as a result( time: 796.5 ) only these hyper parameter values will( time: 797.94 ) be considered for tuning( time: 800.339 ) this list of hyper parameters is not( time: 802.26 ) exhaustive( time: 804.42 ) we are tuning the hyper parameter max( time: 805.92 ) depth within the xgb classifier which is( time: 808.2 ) a maximum tree depth for base learner as( time: 811.86 ) integers of between 2 and 8.( time: 814.74 ) as well as the learning rate the( time: 818.04 ) boosting learning rate as a number( time: 820.92 ) between 0.001 and 1 with log transform( time: 823.019 ) and subsample( time: 827.54 ) and so on until the gamma parameter( time: 829.74 ) you can remove or include more hyper( time: 834.36 ) parameters by reading their definition( time: 836.82 ) within the xgb classifier documentation( time: 838.74 ) which again will be linked in the( time: 842.279 ) description( time: 844.2 ) and you can change their search values( time: 845.279 ) as well( time: 847.139 ) after that we set up a variable opt as( time: 849.24 ) base search CV( time: 853.079 ) and feed it with these( time: 854.82 ) pipe( time: 857.339 ) the pipeline we set up earlier( time: 858.6 ) search space the search space of the( time: 861.18 ) hyper parameters we just defined( time: 863.94 ) CV the number of folds of a cross( time: 866.519 ) validation as three( time: 869.579 ) number of iterations( time: 872.519 ) the number of hyper parameter settings( time: 874.56 ) that are sampled as 10( time: 876.24 ) scoring the metric for evaluation as Roc( time: 878.88 ) AUC( time: 882.959 ) and of course a random state number( time: 884.16 ) in reality you may consider setting CV( time: 887.279 ) and the number of iterations to higher( time: 890.94 ) values to get a better result( time: 893.579 ) we've set them lower so the training( time: 895.8 ) process is faster( time: 897.779 ) note that it's necessary to use a( time: 899.579 ) scikit-learn pipeline when using base( time: 901.92 ) search CV( time: 904.26 ) this ensures our encoding of Target( time: 905.94 ) encoder is being applied to the correct( time: 908.04 ) data set during cross-validation( time: 910.5 ) finally we've got everything set up for( time: 913.44 ) training( time: 915.72 ) step four( time: 918.6 ) train the xgboost model( time: 919.98 ) so opt includes both the pipeline and( time: 922.56 ) the hyper parameter tuning settings( time: 926.22 ) we call its fit method on the training( time: 928.98 ) set( time: 931.32 ) again( time: 932.699 ) there's some warnings that we can ignore( time: 933.959 ) for now( time: 936.06 ) and after waiting we'll have our xtb( time: 938.339 ) models trained( time: 940.56 ) it's done( time: 943.74 ) we'll just click to minimize this long( time: 945.24 ) output( time: 947.1 ) moving on to step 5 evaluate the model( time: 948.6 ) and make predictions( time: 951.54 ) let's look at the chosen pipeline or the( time: 954.0 ) best estimator( time: 956.639 ) you can see these are the columns the( time: 958.5 ) target encoder has encoded( time: 960.42 ) and here is the best xtbook classifier( time: 962.699 ) with these parameter values( time: 965.82 ) now let's evaluate this estimator( time: 969.3 ) if we go back( time: 972.36 ) you can see that we've set up the( time: 974.04 ) scoring within opt as Roc AUC( time: 975.66 ) so going back down( time: 980.339 ) we can call the best score of opt( time: 982.74 ) to see the ROC AUC score for the( time: 985.56 ) training set( time: 988.56 ) the closer the score is to one the( time: 990.18 ) better predictions the model can make( time: 992.519 ) that's a fair score( time: 994.98 ) and by calling the score method on the( time: 997.44 ) test data set( time: 999.6 ) we have the same metric for the test set( time: 1001.04 ) we can see that the scores on the( time: 1005.36 ) training and test sets are close( time: 1007.16 ) to make predictions we use the predict( time: 1011.24 ) or the predict probability methods on( time: 1013.759 ) the test set( time: 1016.279 ) these are the same process as other( time: 1018.5 ) scikit-learn estimators( time: 1020.72 ) all right we're pretty much done the( time: 1023.779 ) last step is optional( time: 1026.419 ) in Step six we'll measure feature( time: 1028.64 ) importance( time: 1030.86 ) we can look at the feature importance if( time: 1032.839 ) you want to interpret the model better( time: 1035.0 ) the xgboost package offers a plotting( time: 1037.52 ) function based on the fitted model( time: 1039.86 ) so first we need to extract the fitted( time: 1043.04 ) xq boost model from opt( time: 1045.439 ) as you can see the xtb classifier is( time: 1048.98 ) printed with this code( time: 1051.86 ) now we can use basic python indexing( time: 1054.559 ) techniques to grab it( time: 1057.38 ) so first within this list( time: 1059.9 ) we use index 1 to grab this part of clf( time: 1062.6 ) then within this Tuple( time: 1068.419 ) we use index one again to grab the model( time: 1070.82 ) this part( time: 1074.539 ) and the code is here( time: 1076.28 ) the xgb classifier model is stored with( time: 1080.36 ) an xgboost model( time: 1083.299 ) we can then apply the plot importance( time: 1085.52 ) function to plot feature importance( time: 1087.74 ) based on such a model( time: 1089.72 ) the default calculates importance as a( time: 1092.299 ) number of times each feature appears in( time: 1095.059 ) a tree( time: 1096.98 ) we can see the feature importance plot( time: 1098.36 ) please investigate more if you're( time: 1101.059 ) interested( time: 1102.86 ) and that's it( time: 1104.179 ) as you can see building xtb models in( time: 1105.98 ) Python is easy( time: 1108.5 ) in this tutorial you successfully built( time: 1110.24 ) an xgboost model and made predictions in( time: 1112.82 ) Python( time: 1114.98 ) did you learn something new in this( time: 1116.419 ) video If so make sure to subscribe to( time: 1118.039 ) our YouTube channel( time: 1120.98 ) just click the Subscribe button below( time: 1122.539 ) this video right now( time: 1124.46 ) if you're interested in more data( time: 1126.38 ) science tutorials and courses please( time: 1127.94 ) head over to our website( time: 1130.1 ) justintadata.com( time: 1131.24 ) thank you and see you in the next video( time: 1133.28 )